{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Empirical Asset Pricing via Machine Learning</center>\n",
    "</h1>\n",
    "\n",
    "\n",
    "\n",
    "<center>Alexander Margetis, Lanya Ma, Sheng Yang, Yiming Tan</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we conducted an empirical analysis of machine learning methods in asset pricing. We aim to measure and predict risk premium with bundles of underlying factors. In particular, we attempt to study the structure of cross-sectional returns by using various factors. These factors can be stock-level firm characteristics, macroeconomic descriptors and many other derived indicators. In the empirical literature, classical models are proposed to estimate and explain the risk premia with several factors, like CAPM, Fama-French 3 factor and later Fama-French 5 factor model. These models are basically linear projection from bahavior of stocks' expected returns to multiple variates. As the high-dimensional nature is innate in machine learning methods, we can enhances the flexibility of representing assets risk profile relative to more traditional econometric prediction techniques.And the functionals which project high-dimensional predictors to risk premia can be complicated. That's why the application of machine learning in this field can be rather attractive.   \n",
    "\n",
    "Our major contributions in this project are three-fold. First, we investigate machine learning techniques in prediction of cross-sectional returns. This tells us whether machine learning algorithms can improve the estimation of out-of-sample expected returns. Second, we examine the feature importance of factors.This process gives us insights that how to select informative factors. Third, we study the stability of machine learning in portfolio constrcution. We analyze the performance of long-short portfolio from the algorithms over the horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning (Lanya Ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3m_tbill_rate.csv', 'bps.csv', 'ca_to_assets.csv', 'current.csv', 'debt_over_asset.csv', 'ebitda_to_sales.csv', 'eps_ttm.csv', 'equity_to_totalcap.csv', 'GICS_code.csv', 'market_cap.csv', 'netprofit_margin.csv', 'ocf_ps.csv', 'ocf_to_debt.csv', 'ocf_to_sales.csv', 'op_to_debt.csv', 'pb.csv', 'pe_ttm.csv', 'price.csv', 'ps_ttm.csv', 'roa.csv', 'roe.csv', 'roic.csv', 'sales_growth1yr.csv', 'tax_to_ebt.csv', 'turnover.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "os.chdir('C:/Users/Jack Tan/Desktop/CFRM/CFRM521Project-master/data')\n",
    "result = glob.glob('*.csv')\n",
    "print(result)\n",
    "data_name_list = list(map(lambda x: x[:-4] , result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def transform_row(filename):\n",
    "    raw_df = pd.read_csv(filename)\n",
    "    row_names = raw_df.iloc[:,0]\n",
    "    raw_df = raw_df.rename(index = row_names)\n",
    "    new_df = raw_df.drop(raw_df.columns[0],axis = 1)\n",
    "    return new_df\n",
    "\n",
    "#Omit stocks w/ no price\n",
    "price = transform_row('price.csv')\n",
    "valid = []\n",
    "nrow = price.shape[0]\n",
    "ncol = price.shape[1]\n",
    "price_mat = np.asmatrix(price)\n",
    "for i in range(0,nrow):\n",
    "    count = 0\n",
    "    for j in range(0,ncol):\n",
    "        if price_mat[i,j] == 0:\n",
    "            count = count+0\n",
    "        else:\n",
    "            count = count+1\n",
    "    if count != 0:\n",
    "        valid.append(i)\n",
    "\n",
    "price_v = price.iloc[valid,:]\n",
    "is_available = copy.deepcopy(price_v)\n",
    "is_available[:] = np.nan\n",
    "is_available[price_v != 0] = 1\n",
    "price_df = price_v*is_available\n",
    "\n",
    "\n",
    "def tranfrom_missing(filename):\n",
    "    raw =  transform_row(filename)\n",
    "    raw = raw.iloc[valid,:]\n",
    "    nrow = raw.shape[0]\n",
    "    ncol = raw.shape[1]\n",
    "    raw = raw.applymap(lambda x: float(x))\n",
    "    raw_mat = np.asmatrix(raw)\n",
    "    for i in range(0,nrow):\n",
    "        fill_data = 0\n",
    "        for j in range(0,ncol):\n",
    "            if raw_mat[i,j] != 0:\n",
    "                fill_data = raw_mat[i,j]\n",
    "            else:\n",
    "                raw_mat[i,j] = fill_data\n",
    "    filled_df = pd.DataFrame(raw_mat)\n",
    "    filled_df.index = price_v.index.values\n",
    "    filled_df.columns = price_v.columns.values\n",
    "    filled_df = np.multiply(filled_df,is_available)\n",
    "    filled_df[filled_df == 0] = np.nan\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "sector_code = transform_row('GICS_code.csv')\n",
    "sector_code[sector_code ==0] = np.nan\n",
    "g_code = sector_code.iloc[:,0].values.reshape(-1, 1)\n",
    "gics_code = g_code[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "for idx, element in enumerate(result):\n",
    "    feature_name = data_name_list[idx]\n",
    "    if feature_name not in ['3m_tbill_rate','GICS_code','price']:\n",
    "        feature_df = tranfrom_missing(element)\n",
    "        features_dict[data_name_list[idx]] = feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock return\n",
    "price_df.head()\n",
    "r_m = copy.deepcopy(price_df)\n",
    "r_m[:] = np.nan\n",
    "\n",
    "nrow = r_m.shape[0]\n",
    "ncol = r_m.shape[1]\n",
    "r_mat = np.asmatrix(r_m)\n",
    "p_mat = np.asmatrix(price_df)\n",
    "for i in range(0,nrow):\n",
    "    for j in range(0,ncol-1):\n",
    "        if np.isnan(p_mat[i,j+1]) or np.isnan(p_mat[i,j]):\n",
    "            r_mat[i,j] = np.nan\n",
    "        else:\n",
    "            r_mat[i,j] = (p_mat[i,j+1]-p_mat[i,j])/p_mat[i,j]\n",
    "return_mon = pd.DataFrame(r_mat)\n",
    "return_mon.index = price_v.index.values\n",
    "return_mon.columns = price_v.columns.values\n",
    "\n",
    "#Risk free rate\n",
    "rf= transform_row('3m_tbill_rate.csv')\n",
    "rfree = rf.loc[return_mon.columns]/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw Data Dictionary\n",
    "data_dict = {}\n",
    "time_keys = price.columns.values\n",
    "first_feature = list(features_dict.keys())[0]\n",
    "for time_key in time_keys[:-1]:\n",
    "    cat_array = features_dict[first_feature][time_key].values.reshape(-1, 1)\n",
    "    for key in list(features_dict.keys())[1:]:\n",
    "        right_array = features_dict[key][time_key].values.reshape(-1, 1)\n",
    "        cat_array = np.concatenate((cat_array,right_array),axis = 1)\n",
    "    if np.isnan(rfree.loc[time_key].values):\n",
    "        rfree.loc[time_key] = 0\n",
    "    y_array = return_mon[time_key].values.reshape(-1,1)-rfree.loc[time_key].values.reshape(-1,1)\n",
    "    cat_array = np.concatenate((cat_array,gics_code),axis = 1)\n",
    "    data_array = np.concatenate((cat_array,y_array),axis = 1)\n",
    "    data_df = pd.DataFrame(data_array)\n",
    "    data_df.index = price_df.index.values\n",
    "    data_df.columns = list(features_dict.keys()) + ['GICS_code'] + ['Excess_return']\n",
    "    data_dict[time_key] = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('factordata.p', 'wb') as fp:\n",
    "    pickle.dump(data_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('factordata.p', 'rb') as fp:\n",
    "    factordata = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct Pipeline\n",
    "def valid_return(dataframe):\n",
    "    exist=[]\n",
    "    nrow = data_df.shape[0]\n",
    "    for i in range(0,nrow):\n",
    "        if not np.isnan(dataframe['Excess_return'][i]):\n",
    "            exist.append(i)\n",
    "    dat = dataframe.iloc[exist,:]\n",
    "    return dat\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imp', IterativeImputer(max_iter=10, random_state=0,estimator = KNeighborsRegressor(n_neighbors=15))),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "        ('one', OneHotEncoder()),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = {}\n",
    "y = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_key in time_keys[:-1]:\n",
    "    dat = valid_return(data_dict[time_key])\n",
    "    X_raw = dat.drop('Excess_return', axis=1)\n",
    "    num_features = X_raw.drop('GICS_code', axis=1)\n",
    "    num_attribs = list(num_features)\n",
    "    cat_attribs = [\"GICS_code\"]\n",
    "    full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "    X_array = full_pipeline.fit_transform(dat)\n",
    "    y_array = dat['Excess_return'].values\n",
    "    X[time_key] = pd.DataFrame(X_array)\n",
    "    y[time_key] = pd.DataFrame(y_array)\n",
    "    y[time_key].columns = ['Excess_return']\n",
    "    y[time_key].index = dat.index.values\n",
    "    X[time_key].index = dat.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = {}\n",
    "Test = {}\n",
    "for time_key in time_keys[:100]:\n",
    "    Train[time_key] = pd.concat([X[time_key],y[time_key]],axis=1)\n",
    "for time_key in time_keys[100:-1]:\n",
    "    Test[time_key] = pd.concat([X[time_key],y[time_key]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cleanTrain.p', 'wb') as gp:\n",
    "    pickle.dump(Train, gp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cleanTest.p', 'wb') as hp:\n",
    "    pickle.dump(Test, hp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generalized linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Support vector machines \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Dimension Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Description and Exploratory Data Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset in this project includes all listed firms in the NYSE, AMEX, and NASDAQ. Our sample begins in January 2009 to April 2019. Our data is monthly updated. We use the Treasury-bill rate as risk-free rate for calculating the excessive returns. The firms characteristics or the factors in other words, include firms' value, growth, solvency, cash flow, profitability, operating capacity, capital structure and momentum. In addition, we include the categorical industry classes corresponding to GICS sectors.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Out-of-sample Stock-level Prediction Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Variable Importance for factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Machine Learning Portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gu, Shihao, Bryan Kelly, and Dacheng Xiu. *Empirical asset pricing via machine learning.* No. w25398. National Bureau of Economic Research, 2018.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
