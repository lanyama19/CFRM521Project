{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Empirical Asset Pricing via Machine Learning</center>\n",
    "</h1>\n",
    "\n",
    "\n",
    "\n",
    "<center>Alexander Margetis, Lanya Ma, Sheng Yang, Yiming Tan</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we conducted an empirical analysis of machine learning methods in asset pricing. We aim to measure and predict risk premium with bundles of underlying factors. In particular, we attempt to study the structure of cross-sectional returns by using various factors. These factors can be stock-level firm characteristics, macroeconomic descriptors and many other derived indicators. In the empirical literature, classical models are proposed to estimate and explain the risk premia with several factors, like CAPM, Fama-French 3 factor and later Fama-French 5 factor model. These models are basically linear projection from bahavior of stocks' expected returns to multiple variates. As the high-dimensional nature is innate in machine learning methods, we can enhances the flexibility of representing assets risk profile relative to more traditional econometric prediction techniques.And the functionals which project high-dimensional predictors to risk premia can be complicated. That's why the application of machine learning in this field can be rather attractive.   \n",
    "\n",
    "Our major contributions in this project are three-fold. First, we investigate machine learning techniques in prediction of cross-sectional returns. This tells us whether machine learning algorithms can improve the estimation of out-of-sample expected returns. Second, we examine the feature importance of factors.This process gives us insights that how to select informative factors. Third, we study the stability of machine learning in portfolio constrcution. We analyze the performance of long-short portfolio from the algorithms over the horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing and Exploratory Data Analysis (Lanya Ma and Yiming Tan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning (Lanya Ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3m_tbill_rate.csv', 'bps.csv', 'ca_to_assets.csv', 'current.csv', 'debt_over_asset.csv', 'ebitda_to_sales.csv', 'eps_ttm.csv', 'equity_to_totalcap.csv', 'GICS_code.csv', 'market_cap.csv', 'netprofit_margin.csv', 'ocf_ps.csv', 'ocf_to_debt.csv', 'ocf_to_sales.csv', 'op_to_debt.csv', 'pb.csv', 'pe_ttm.csv', 'price.csv', 'ps_ttm.csv', 'roa.csv', 'roe.csv', 'roic.csv', 'sales_growth1yr.csv', 'tax_to_ebt.csv', 'turnover.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(os.getcwd() + '\\\\data')\n",
    "extension = 'csv'\n",
    "result = glob.glob('*.{}'.format(extension))\n",
    "print(result)\n",
    "data_name_list = list(map(lambda x: x[:-4] , result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def transform_row(filename):\n",
    "    raw_df = pd.read_csv(filename)\n",
    "    row_names = raw_df.iloc[:,0]\n",
    "    raw_df = raw_df.rename(index = row_names)\n",
    "    new_df = raw_df.drop(raw_df.columns[0],axis = 1)\n",
    "    return new_df\n",
    "\n",
    "#Omit stocks w/ no price\n",
    "price = transform_row('price.csv')\n",
    "sum_df = pd.DataFrame(price.sum(axis = 1)).reset_index()\n",
    "valid = sum_df[sum_df[0]!=0].index.values\n",
    "\n",
    "price_v = price.iloc[valid,:]\n",
    "is_available = copy.deepcopy(price_v)\n",
    "is_available[:] = np.nan\n",
    "is_available[price_v != 0] = 1\n",
    "price_df = price_v*is_available\n",
    "\n",
    "\n",
    "def tranfrom_missing(filename):\n",
    "    raw =  transform_row(filename)\n",
    "    raw = raw.iloc[valid,:]\n",
    "    nrow = raw.shape[0]\n",
    "    ncol = raw.shape[1]\n",
    "    raw = raw.applymap(lambda x: float(x))\n",
    "    raw_mat = np.asmatrix(raw)\n",
    "    for i in range(0,nrow):\n",
    "        fill_data = 0\n",
    "        for j in range(0,ncol):\n",
    "            if raw_mat[i,j] != 0:\n",
    "                fill_data = raw_mat[i,j]\n",
    "            else:\n",
    "                raw_mat[i,j] = fill_data\n",
    "    filled_df = pd.DataFrame(raw_mat)\n",
    "    filled_df.index = price_v.index.values\n",
    "    filled_df.columns = price_v.columns.values\n",
    "    filled_df = np.multiply(filled_df,is_available)\n",
    "    filled_df[filled_df == 0] = np.nan\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "sector_code = transform_row('GICS_code.csv')\n",
    "sector_code[sector_code ==0] = np.nan\n",
    "\"\"\"\n",
    "sector_intcode = []\n",
    "for code in sector_code['GICS_Code']:\n",
    "    if ~np.isnan(code):\n",
    "        code = int(code)\n",
    "    sector_intcode.append(code)\n",
    "\"\"\"\n",
    "\n",
    "# Consider S&P Dow Jones Indices and MSCI changed sector classification\n",
    "sector_dict = {1010:\"Energy\", 1510:\"Materials\",2010:\"Capital Goods\", 2020:\"Commercial & Professional Services\",\n",
    "               2030:\"Transportation\", 2510:\"Automobiles & Components\", 2520:\"Consumer Durables & Apparel\",\n",
    "               2530:\"Consumer Services\", 2540:\"Media & Entertainment\", 2550:\"Retailing\", 3010:\"Food & Staples Retailing\",\n",
    "               3020:\"Food, Beverage & Tobacco\", 3030:\"Household & Personal Products\", 3510:\"Health Care Equipment & Services\",\n",
    "               3520:\"Pharmaceuticals, Biotechnology & Life Sciences\", 4010:\"Banks\", 4020:\"Diversified Financials\",\n",
    "               4030:\"Insurance\", 4040:\"Real Estate\",4510:\"Software & Services\", 4520:\"Technology Hardware & Equipment\",\n",
    "               4530:\"Semiconductors & Semiconductor Equipment\", 5010:\"Telecommunication Services\", 5020:\"Media & Entertainment\",\n",
    "               5510:\"Utilities\", 6010:\"Real Estate\"}    \n",
    "\n",
    "sectors_list = []\n",
    "for code in sector_code['GICS_Code']:\n",
    "    if ~np.isnan(code):\n",
    "        code = sector_dict[code]\n",
    "    sectors_list.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "numeric_features = []\n",
    "for idx, element in enumerate(result):\n",
    "    feature_name = data_name_list[idx]\n",
    "    if feature_name not in ['3m_tbill_rate','GICS_code','price']:\n",
    "        feature_df = tranfrom_missing(element)\n",
    "        features_dict[data_name_list[idx]] = feature_df\n",
    "        numeric_features.append(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock return\n",
    "return_mon = (price_df.T.shift(1).T - price_df)/price_df\n",
    "# shift the next month return ahead so that we can concatenate features and target\n",
    "return_mon = return_mon.T.shift(-1).T\n",
    "#Risk free rate\n",
    "rf= transform_row('3m_tbill_rate.csv')\n",
    "rfree = rf['rate'][return_mon.columns.values]/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw Data Dictionary\n",
    "data_dict = {}\n",
    "time_keys = price.columns.values\n",
    "first_feature = list(features_dict.keys())[0]\n",
    "sectors_array = np.array(sectors_list).reshape(-1, 1)\n",
    "sectors_array = sectors_array[valid]\n",
    "for time_key in time_keys[:-1]:\n",
    "    cat_array = features_dict[first_feature][time_key].values.reshape(-1, 1)\n",
    "    for key in list(features_dict.keys())[1:]:\n",
    "        right_array = features_dict[key][time_key].values.reshape(-1, 1)\n",
    "        cat_array = np.concatenate((cat_array,right_array),axis = 1)\n",
    "    y_array = return_mon[time_key].values.reshape(-1,1)-rfree[time_key]\n",
    "    cat_array = np.concatenate((cat_array,sectors_array),axis = 1)\n",
    "    data_array = np.concatenate((cat_array,y_array),axis = 1)\n",
    "    data_df = pd.DataFrame(data_array)\n",
    "    data_df.index = price_df.index.values\n",
    "    data_df.columns = list(features_dict.keys()) + ['Sectors'] + ['Excess_return']\n",
    "    data_dict[time_key] = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('factordata.p', 'wb') as fp:\n",
    "    pickle.dump(data_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('factordata.p', 'rb') as fp:\n",
    "    factordata = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data pipeline(Yiming Tan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_return(dataframe):\n",
    "    # will raise error with object type in nparray\n",
    "    \n",
    "    nrow = data_df.shape[0]\n",
    "    for i in range(0,nrow):\n",
    "        if not np.isnan(dataframe['Excess_return'][i]):\n",
    "            exist.append(i)\n",
    "    dat = dataframe.iloc[exist,:]\n",
    "    return dat\n",
    "\n",
    "def valid_returnV2(dataframe):\n",
    "    dataframe['Excess_return'] = dataframe['Excess_return'].astype(float)\n",
    "    # exclude rows where Excess_return is nan\n",
    "    dat = dataframe[np.isfinite(dataframe['Excess_return'])]\n",
    "    return dat\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imp', IterativeImputer(max_iter=10, random_state=0,estimator = KNeighborsRegressor(n_neighbors=15))),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "        ('one', OneHotEncoder(categories='auto')),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = {}\n",
    "y = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Automobiles & Components',\n",
       " 'Banks',\n",
       " 'Capital Goods',\n",
       " 'Commercial & Professional Services',\n",
       " 'Consumer Durables & Apparel',\n",
       " 'Consumer Services',\n",
       " 'Diversified Financials',\n",
       " 'Energy',\n",
       " 'Food & Staples Retailing',\n",
       " 'Food, Beverage & Tobacco',\n",
       " 'Health Care Equipment & Services',\n",
       " 'Household & Personal Products',\n",
       " 'Insurance',\n",
       " 'Materials',\n",
       " 'Media & Entertainment',\n",
       " 'Pharmaceuticals, Biotechnology & Life Sciences',\n",
       " 'Real Estate',\n",
       " 'Retailing',\n",
       " 'Semiconductors & Semiconductor Equipment',\n",
       " 'Software & Services',\n",
       " 'Technology Hardware & Equipment',\n",
       " 'Telecommunication Services',\n",
       " 'Transportation',\n",
       " 'Utilities'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing sector with most_frequent item\n",
    "cat_counts = np.unique(sectors_array, return_counts=True)\n",
    "most_freq_sec = cat_counts[0][np.argmax(cat_counts[1])]\n",
    "sectors_array[sectors_array == 'nan'] = most_freq_sec\n",
    "set(sectors_array.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "encoded_array = onehotencoder.fit_transform(sectors_array.reshape(-1,1)).toarray()\n",
    "dummy_names = onehotencoder.get_feature_names().astype(str)\n",
    "dummy_names = list(map(lambda x: x[3:] , dummy_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_key in time_keys[:-1]:\n",
    "    raw_df = data_dict[time_key].drop('Sectors', axis=1)\n",
    "    complete_array = np.concatenate((raw_df,encoded_array),axis=1)\n",
    "    complete_df = pd.DataFrame(complete_array)\n",
    "    whole_col_names = numeric_features + ['Excess_return'] + dummy_names\n",
    "    complete_df.columns = whole_col_names\n",
    "    dat = valid_returnV2(complete_df)\n",
    "    num_features = dat.drop('Excess_return',axis=1).iloc[:,:len(numeric_features)]\n",
    "    cat_features = dat.iloc[:, len(numeric_features)+1:]\n",
    "    num_features_ = num_pipeline.fit_transform(num_features)\n",
    "    X_array = np.concatenate((num_features_,cat_features),axis=1)\n",
    "    X[time_key] = pd.DataFrame(X_array)\n",
    "    X[time_key].index = dat.index.values\n",
    "    X[time_key].columns = numeric_features + dummy_names\n",
    "    y[time_key] = pd.DataFrame(dat['Excess_return'])\n",
    "    y[time_key].index = dat.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = {}\n",
    "Test = {}\n",
    "for time_key in time_keys[:100]:\n",
    "    Train[time_key] = pd.concat([X[time_key],y[time_key]],axis=1)\n",
    "for time_key in time_keys[100:-1]:\n",
    "    Test[time_key] = pd.concat([X[time_key],y[time_key]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cleanTrain.p', 'wb') as gp:\n",
    "    pickle.dump(Train, gp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cleanTest.p', 'wb') as hp:\n",
    "    pickle.dump(Test, hp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generalized linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Support vector machines \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "os.chdir('C:/Users/Jack Tan/Desktop/CFRM521Project-master/data')\n",
    "\n",
    "pickle_off_x = open(\"XData.p\",\"rb\")\n",
    "X_raw = pickle.load(pickle_off_x)\n",
    "pickle_off_y = open(\"yData.p\",\"rb\")\n",
    "y_raw = pickle.load(pickle_off_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame()\n",
    "X = pd.DataFrame()\n",
    "for key in y_raw.keys():\n",
    "    if key in ('12/31/2008'):\n",
    "        y = y_raw[key]\n",
    "        X = X_raw[key]\n",
    "    else:\n",
    "        y = pd.concat([y,y_raw[key]])\n",
    "        X = pd.concat([X,X_raw[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import newaxis\n",
    "X_train_full = X[:460560].values\n",
    "y_train_full = y[:460560].values\n",
    "X_valid = X_train_full[:46056][:, :, newaxis]\n",
    "y_valid = np.concatenate((y_train_full[:46056]), axis=None)\n",
    "X_train = X_train_full[46056:460560][:, :, newaxis]\n",
    "y_train = np.concatenate((y_train_full[46056:460560]), axis=None)\n",
    "X_test = X[460560:].values[:, :, newaxis]\n",
    "y_test = np.concatenate((y[460560:].values), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "reset_session()\n",
    "MyDense = partial(keras.layers.Dense,\n",
    "                  activation = \"relu\", kernel_initializer=\"normal\")\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[46, 1]))\n",
    "for n_hidden in (300, 100, 100, 50, 50, 50):\n",
    "    model.add(MyDense(n_hidden))\n",
    "model.add(keras.layers.Dense(1, activation=\"linear\", name=\"Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"mean_squared_error\"])\n",
    "model.save(\"First\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 414504 samples, validate on 46056 samples\n",
      "Epoch 1/200\n",
      "414504/414504 [==============================] - 36s 86us/sample - loss: 0.4047 - mean_squared_error: 0.4047 - val_loss: 29.9468 - val_mean_squared_error: 29.9468\n",
      "Epoch 2/200\n",
      "414504/414504 [==============================] - 33s 81us/sample - loss: 0.4047 - mean_squared_error: 0.4047 - val_loss: 29.9475 - val_mean_squared_error: 29.9475\n",
      "Epoch 3/200\n",
      "414504/414504 [==============================] - 30s 72us/sample - loss: 0.4044 - mean_squared_error: 0.4044 - val_loss: 29.9459 - val_mean_squared_error: 29.9459\n",
      "Epoch 4/200\n",
      "414504/414504 [==============================] - 30s 72us/sample - loss: 0.4043 - mean_squared_error: 0.4043 - val_loss: 29.9462 - val_mean_squared_error: 29.9462\n",
      "Epoch 5/200\n",
      "414504/414504 [==============================] - 30s 72us/sample - loss: 0.4042 - mean_squared_error: 0.4042 - val_loss: 29.9462 - val_mean_squared_error: 29.9462\n",
      "Epoch 6/200\n",
      "414504/414504 [==============================] - 31s 74us/sample - loss: 0.4040 - mean_squared_error: 0.4040 - val_loss: 29.9460 - val_mean_squared_error: 29.9460\n",
      "Epoch 7/200\n",
      "414504/414504 [==============================] - 31s 76us/sample - loss: 0.4039 - mean_squared_error: 0.4039 - val_loss: 29.9469 - val_mean_squared_error: 29.9469\n",
      "Epoch 8/200\n",
      "414504/414504 [==============================] - 32s 77us/sample - loss: 0.4034 - mean_squared_error: 0.4034 - val_loss: 29.9464 - val_mean_squared_error: 29.9463\n",
      "Epoch 9/200\n",
      "414504/414504 [==============================] - 30s 73us/sample - loss: 0.4038 - mean_squared_error: 0.4038 - val_loss: 29.9469 - val_mean_squared_error: 29.9469\n",
      "Epoch 10/200\n",
      "414504/414504 [==============================] - 34s 82us/sample - loss: 0.4036 - mean_squared_error: 0.4036 - val_loss: 29.9464 - val_mean_squared_error: 29.9464\n",
      "Epoch 11/200\n",
      "414504/414504 [==============================] - 30s 72us/sample - loss: 0.4035 - mean_squared_error: 0.4035 - val_loss: 29.9466 - val_mean_squared_error: 29.9466\n"
     ]
    }
   ],
   "source": [
    "reset_session()\n",
    "model = keras.models.load_model(\"First\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"First\",\n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  min_delta=0.001,\n",
    "                                                  restore_best_weights=True)\n",
    "run = model.fit(X_train, y_train, epochs=200,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30971623014909866"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "mean_squared_error(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Dimension Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Description and Exploratory Data Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset in this project includes all listed firms in the NYSE, AMEX, and NASDAQ. Our sample begins in January 2009 to April 2019. Our data is monthly updated. We use the Treasury-bill rate as risk-free rate for calculating the excessive returns. The firms characteristics or the factors in other words, include firms' value, growth, solvency, cash flow, profitability, operating capacity, capital structure and momentum. In addition, we include the categorical industry classes corresponding to GICS sectors.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Out-of-sample Stock-level Prediction Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Variable Importance for factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Machine Learning Portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gu, Shihao, Bryan Kelly, and Dacheng Xiu. *Empirical asset pricing via machine learning.* No. w25398. National Bureau of Economic Research, 2018.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
